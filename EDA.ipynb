{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "designing-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "colored-thailand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22783785, 9)\n",
      "cust_id\n",
      "event_start\n",
      "event_type\n",
      "rate_plan_id\n",
      "flag_1\n",
      "flag_2\n",
      "duration\n",
      "charge\n",
      "month\n",
      "\n",
      "resized:\n",
      "(2278378, 9)\n"
     ]
    }
   ],
   "source": [
    "usage_data = pd.read_csv('usage.csv', names=['cust_id', 'event_start', 'event_type', 'rate_plan_id', 'flag_1', 'flag_2', 'duration', 'charge', 'month'])\n",
    "print(usage_data.shape)\n",
    "for col in usage_data.columns:\n",
    "    print(col)\n",
    "    \n",
    "# randomly downscale the data for the purpose of analysing it more efficiently.\n",
    "# the distribution of data and correlations between some attributes remains\n",
    "# the same as for original dataset, since the dataset is downscaled randomly.\n",
    "# (the whole dataset should be used for data analysis, however, due to the\n",
    "# limited computational resources, the fraction of dataset had to be used)\n",
    "usage_data = usage_data.sample(frac=0.1, replace=True, random_state=1)\n",
    "print(\"\\nresized:\")\n",
    "print(usage_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "threaded-dealing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cust_id         0.0\n",
      "event_start     0.0\n",
      "event_type      0.0\n",
      "rate_plan_id    0.0\n",
      "flag_1          0.0\n",
      "flag_2          0.0\n",
      "duration        0.0\n",
      "charge          0.0\n",
      "month           0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# look for the damaged data, such as the elements containing null values.\n",
    "# as we can see, the data fully contains values, as the missing percentage\n",
    "# of data for each row is 0%\n",
    "# if some attributes contain empty values, the billing provider has to be notified ASAP\n",
    "# for damaged data. the data itself has to be cleansed.\n",
    "missing_perc = usage_data.isnull().sum()*100/len(usage_data)\n",
    "print(missing_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seeing-fourth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lists are equal: True\n"
     ]
    }
   ],
   "source": [
    "# another thing that is worth observing is that the 'month' attribute in the\n",
    "# provided dataset contains the same information as the 'event_start' field.\n",
    "# it's safe to assume that we can drop the 'month' column as we can get\n",
    "# the same information from 'event_start'.\n",
    "\n",
    "\n",
    "# exctract month information from 'event_start' field in form YYYY-MM like\n",
    "# it is defined in 'month' attribute.\n",
    "event_start_month = usage_data['event_start'].apply(lambda x : x[:7])\n",
    "\n",
    "# compare whether the values in both lists are equal\n",
    "print('lists are equal:',(usage_data['month'].tolist() == event_start_month).all())\n",
    "\n",
    "\n",
    "# drop the 'month' column of the dataframe for further analysis.\n",
    "# note that this attribute will be removed when loading to the database\n",
    "usage_data = usage_data.drop(['month'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recreational-holiday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approximately 5% of data is duplicated\n",
      "after deletion: 0% duplicated data\n"
     ]
    }
   ],
   "source": [
    "# continuing looking for data inconsistency and its quality we find that\n",
    "# the data contains duplications. Thus, we will remove the duplicated rows\n",
    "# prior to the uploading to the warehouse.\n",
    "duplicatedRows = usage_data[usage_data.duplicated()]\n",
    "\n",
    "print('approximately ' +str(round(len(duplicatedRows)/len(usage_data)*100))+'% of data is duplicated')\n",
    "\n",
    "# drop the duplicated rows\n",
    "usage_data = usage_data.drop_duplicates()\n",
    "\n",
    "duplicatedRows = usage_data[usage_data.duplicated()]\n",
    "\n",
    "print('after deletion: ' +str(round(len(duplicatedRows)/len(usage_data)*100))+'% duplicated data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "norwegian-settle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2166562, 8)\n",
      "cust_id            9460\n",
      "event_start     1841464\n",
      "event_type            4\n",
      "rate_plan_id       2329\n",
      "flag_1              207\n",
      "flag_2                2\n",
      "duration          10204\n",
      "charge            82306\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# checkcing the data shape and the unique values count for each attribute.\n",
    "\n",
    "print(usage_data.shape)\n",
    "\n",
    "print(usage_data.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "western-anthropology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- cust_id\n",
      "min value: 1609  max value: 2027509  mean: 1170194.8485886857\n",
      "- rate_plan_id\n",
      "min value: 0  max value: 42705  mean: 21679.08809164012\n",
      "- flag_1\n",
      "min value: 0  max value: 234  mean: 6.045190029179871\n",
      "- flag_2\n",
      "min value: 0  max value: 1  mean: 0.2339125305437832\n",
      "- duration\n",
      "min value: 0  max value: 18522  mean: 750.7732356609228\n",
      "- charge\n",
      "min value: 0.0  max value: 1496.598432  mean: 0.6891174207661719\n",
      "- event_type\n",
      "possible values: ['VOICE' 'DATA' 'SMS' 'MMS']\n"
     ]
    }
   ],
   "source": [
    "# analyse the existing values for new data insertion. if we know the expected\n",
    "# range or values for the attributes, we can constrain and reject records that\n",
    "# exceeds our expectations. also, some errors in data might be found. \n",
    "# for example, if we receive a record with 'event_type' value that is different\n",
    "# from the existing 4 classes, it means our record might be damaged and needs\n",
    "# to be cleansed.\n",
    "\n",
    "for col in usage_data.columns:\n",
    "    if usage_data[col].dtype == np.float64 or usage_data[col].dtype == np.int64:\n",
    "        print('-',col)\n",
    "        print('min value:', min(usage_data[col]), ' max value:', max(usage_data[col]), ' mean:', usage_data[col].mean())\n",
    "\n",
    "print('-', 'event_type')\n",
    "print('possible values:', usage_data['event_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "signed-external",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cust_id                                827919\n",
      "event_start     2016-02-12T21:50:27.000+02:00\n",
      "event_type                              VOICE\n",
      "rate_plan_id                            16538\n",
      "flag_1                                      1\n",
      "flag_2                                      1\n",
      "duration                                  984\n",
      "charge                                    0.0\n",
      "Name: 12710949, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# after performing this minimal analysis of existing data, we can\n",
    "# see that it can be expected to receive some duplicated data which\n",
    "# should be removed prior to loading. also, we might as well avoid using\n",
    "# the 'month' attribute as 'event_start' contains the same information, thus,\n",
    "# will allow us save space.\n",
    "\n",
    "# depending on our expectations, we can constraint the values of new data\n",
    "# for example, limiting the 'event_type' field to currently existing 4 classes,\n",
    "# or allowing maximum value of 'flag_1' to be 234 as it currently is. otherwise,\n",
    "# mark the record as damaged, notify the provider and cleanse the data.\n",
    "\n",
    "print(usage_data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-trademark",
   "metadata": {},
   "source": [
    "# Designing data structure for the Data Warehouse\n",
    "\n",
    "Once the ETL is performed, the Data Marts will have to be created in the database as was requested by the Product guys. Firstly, after the data validation, cleansing and removal of unnecessary records the data will be uploaded to the warehouse in the initial structure as was provided within the csv file, only without the 'month' attribute. The overall structure of data will be stored in the following form:\n",
    "\n",
    "|cust_id|event_start|event_type|rate_plan_id|flag_1|flag_2|duration|charge|\n",
    "|------|------|------|------|------|------|------|------|\n",
    "|827919  |2016-02-...  |VOICE  | 16538 | 1 | 1 | 984 | 0.0 |\n",
    "|...  |...  |...  | ... | ... | ... | ... | ... |\n",
    "\n",
    "As was asked by the Product guys, the distribution of different service types (event_type) and rate plans (rate_plan_id) has to be provided together with the number of customers for each respective field. Thus, two Data Marts were designed to allow for efficient and simple retrieval of such information. \n",
    "\n",
    "Data Mart for service types contains 4 rows, each for a different service type, and 3 columns, where first column is a primary key of event_type, second column contains number of existing projects of voice type (usage distribution) and third column - number of distinct customers using such service type.\n",
    "\n",
    "|event_type|project_count|customers|\n",
    "|------|------|------|\n",
    "|VOICE  |distribution of VOICE projects  |#of customers  |\n",
    "|DATA  |distribution of DATA projects  |#of customers  |\n",
    "|SMS  |distribution of SMS projects  |#of customers  |\n",
    "|MMS  |distribution of MMS projects  |#of customers  |\n",
    "\n",
    "The table for the Rate Plan is designed in the same way, only the distribution and number of customers are counted for each Rate Plan (rate_plan_id as a primary key).\n",
    "\n",
    "|rate_plan_id|project_count|customers|\n",
    "|------|------|------|\n",
    "|0  |distribution of rate plan 0  |#of customers  |\n",
    "|1  |distribution of rate plan 1  |#of customers  |\n",
    "|...  |...  |...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-settle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
